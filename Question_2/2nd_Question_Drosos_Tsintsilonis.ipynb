{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e844deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: (111795, 4) rows\n",
      "Test Data: (47912, 3) rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import scipy.sparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Load the test dataset\n",
    "test_unlabeled_df = pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "print(f\"Training Data: {train_df.shape} rows\")\n",
    "print(f\"Test Data: {test_unlabeled_df.shape} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e1ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions are ready!\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    \n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation and numbers\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to preprocess entire dataset\n",
    "def preprocess_dataset(df, text_columns):\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].astype(str).apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "print(\"Preprocessing functions are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a693792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "\n",
      "Processing column: Title\n",
      "Progress: 100.00%\n",
      "Processing column: Content\n",
      "Progress: 100.00%\n",
      "Preprocessing complete!\n",
      "Combining Title and Content into a single field...\n",
      "Combining complete!\n",
      "Preprocessed data saved: preprocessed_train_2nd.csv, preprocessed_test_2nd.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "text_columns = ['Title', 'Content']\n",
    "print(\"Preprocessing text data...\")\n",
    "\n",
    "for col in text_columns:\n",
    "    print(f\"\\nProcessing column: {col}\")\n",
    "    \n",
    "    total_rows = len(train_df) + len(test_unlabeled_df)  # Total rows to process\n",
    "    processed_rows = 0  # Track processed rows\n",
    "\n",
    "    # Process training data\n",
    "    for i in range(len(train_df)):\n",
    "        train_df.at[i, col] = preprocess_text(str(train_df.at[i, col]))\n",
    "        processed_rows += 1\n",
    "        percentage = (processed_rows / total_rows) * 100\n",
    "        sys.stdout.write(f\"\\rProgress: {percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Process test data\n",
    "    for i in range(len(test_unlabeled_df)):\n",
    "        test_unlabeled_df.at[i, col] = preprocess_text(str(test_unlabeled_df.at[i, col]))\n",
    "        processed_rows += 1\n",
    "        percentage = (processed_rows / total_rows) * 100\n",
    "        sys.stdout.write(f\"\\rProgress: {percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "\n",
    "# Combine 'Title' and 'Content' into a single field\n",
    "print(\"Combining Title and Content into a single field...\")\n",
    "train_df['Combined'] = train_df['Title'] + ' ' + train_df['Content']\n",
    "test_unlabeled_df['Combined'] = test_unlabeled_df['Title'] + ' ' + test_unlabeled_df['Content']\n",
    "\n",
    "print(\"Combining complete!\")\n",
    "\n",
    "# Save preprocessed data\n",
    "train_df.to_csv(\"preprocessed_train_2nd.csv\", index=False)\n",
    "test_unlabeled_df.to_csv(\"preprocessed_test_2nd.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessed data saved: preprocessed_train_2nd.csv, preprocessed_test_2nd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9210828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets...\n",
      "Train set size: 111795, Columns: ['Id', 'Title', 'Content', 'Label', 'Combined']\n",
      "Test set size: 47912, Columns: ['Id', 'Title', 'Content', 'Combined']\n",
      "100% of the dataset is used.\n",
      "Reduced Train set size: 111795, Reduced Test set size: 47912\n",
      "Vectorized data files found. Do you want to use them? (yes/no): yes\n",
      "Loading precomputed vectorized data...\n",
      "Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load Preprocessed Data\n",
    "print(\"Loading preprocessed datasets...\")\n",
    "train_df = pd.read_csv(\"preprocessed_train_2nd.csv\")\n",
    "test_df = pd.read_csv(\"preprocessed_test_2nd.csv\")\n",
    "#print(f\"Train set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
    "print(f\"Train set size: {len(train_df)}, Columns: {list(train_df.columns)}\")\n",
    "print(f\"Test set size: {len(test_df)}, Columns: {list(test_df.columns)}\")\n",
    "\n",
    "\n",
    "# Reduce dataset size for faster processing\n",
    "subset_size = 1  # Use % of the dataset\n",
    "train_df_reduced = train_df.sample(frac=subset_size, random_state=42)\n",
    "test_df_reduced = test_df.sample(frac=subset_size, random_state=42)\n",
    "print(f\"{subset_size * 100:.0f}% of the dataset is used.\")\n",
    "print(f\"Reduced Train set size: {len(train_df_reduced)}, Reduced Test set size: {len(test_df_reduced)}\")\n",
    "\n",
    "# Check if vectorized files exist and ask user for input\n",
    "if os.path.exists(\"train_vectors.npz\") and os.path.exists(\"test_vectors.npz\"):\n",
    "    user_input = input(\"Vectorized data files found. Do you want to use them? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if user_input == \"yes\":\n",
    "        print(\"Loading precomputed vectorized data...\")\n",
    "        train_vectors = scipy.sparse.load_npz(\"train_vectors.npz\")\n",
    "        test_vectors = scipy.sparse.load_npz(\"test_vectors.npz\")\n",
    "    else:\n",
    "        print(\"Recomputing vectorized data...\")\n",
    "        vectorizer = CountVectorizer(binary=True, analyzer='word', ngram_range=(1, 1), max_features=5000)\n",
    "        train_vectors = vectorizer.fit_transform(train_df_reduced['Combined'])\n",
    "        test_vectors = vectorizer.transform(test_df_reduced['Combined'])\n",
    "        \n",
    "        # Save vectorized data for future use\n",
    "        scipy.sparse.save_npz(\"train_vectors.npz\", train_vectors)\n",
    "        scipy.sparse.save_npz(\"test_vectors.npz\", test_vectors)\n",
    "        print(\"Vectorized data saved.\")\n",
    "else:\n",
    "    print(\"Vectorized data files not found. Computing new vectorized data...\")\n",
    "    # Use of CountVectorizer with binary=True for Jaccard similarity\n",
    "    vectorizer = CountVectorizer(\n",
    "        analyzer='word', \n",
    "        ngram_range=(1, 1),  # Use only unigrams\n",
    "        max_features=5000,   # Reduce dimensionality\n",
    "        binary=True          # Convert to binary (1 if word appears, 0 if not)\n",
    "    )    \n",
    "    \n",
    "    train_vectors = vectorizer.fit_transform(train_df_reduced['Combined'])\n",
    "    test_vectors = vectorizer.transform(test_df_reduced['Combined'])\n",
    "    \n",
    "    # Save vectorized data for future use\n",
    "    scipy.sparse.save_npz(\"train_vectors.npz\", train_vectors)\n",
    "    scipy.sparse.save_npz(\"test_vectors.npz\", test_vectors)\n",
    "    print(\"Vectorized data saved.\")\n",
    "\n",
    "print(\"Preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a933f7-efce-4225-8a49-4e844e7ca691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorized data...\n",
      "Vectorized data loaded.\n",
      "Brute-Force K-NN results found. Do you want to use them? (yes/no): yes\n",
      "Loading precomputed Brute-Force results...\n",
      "Sample of True K-NN Results (First 10 test samples):\n",
      "Test sample 0 K-NN: [  8974  36912  29772 105602  77486  93430  71663]\n",
      "Test sample 1 K-NN: [50059 42849 68395 17059 49499 64408 27661]\n",
      "Test sample 2 K-NN: [ 35822  88254  80655   9421  20744  43597 101570]\n",
      "Test sample 3 K-NN: [ 81865 106753  11904  18248    603  23201   6025]\n",
      "Test sample 4 K-NN: [55241 23134 75246  8489 61535 45757  7065]\n",
      "Test sample 5 K-NN: [ 69811  26604 110408  93141  34844  87105  72534]\n",
      "Test sample 6 K-NN: [ 54059 100699  29993 110148 108542  14328  90384]\n",
      "Test sample 7 K-NN: [58272  5892 18340 83212 96691 29713 71955]\n",
      "Test sample 8 K-NN: [ 71262 101775  74176  22345  86448  60058 103796]\n",
      "Test sample 9 K-NN: [ 70466  95604  26830 111625  39114  67278 100622]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Load Vectorized Data\n",
    "print(\"Loading vectorized data...\")\n",
    "train_vectors = scipy.sparse.load_npz(\"train_vectors.npz\").toarray().astype(bool) \n",
    "test_vectors = scipy.sparse.load_npz(\"test_vectors.npz\").toarray().astype(bool) \n",
    "print(\"Vectorized data loaded.\")\n",
    "\n",
    "# Brute-Force K-NN with Jaccard similarity\n",
    "def brute_force_knn_optimized(train_vectors, test_vectors, k=7):\n",
    "    \"\"\"Optimized Brute-Force K-NN using Jaccard similarity.\"\"\"\n",
    "    print(\"Starting optimized brute-force evaluation...\")\n",
    "\n",
    "    true_knn = np.zeros((test_vectors.shape[0], k), dtype=int)\n",
    "    brute_start_time = time.time()\n",
    "\n",
    "    # Compute distances row by row\n",
    "    for i in tqdm(range(test_vectors.shape[0]), desc=\"Computing K-NN\", unit=\"doc\"):\n",
    "        distances = pairwise_distances(test_vectors[i].reshape(1, -1), train_vectors, metric=\"jaccard\")\n",
    "        knn_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "        true_knn[i] = knn_indices\n",
    "\n",
    "    brute_force_time = time.time() - brute_start_time\n",
    "    print(f\"\\nBrute-Force K-NN completed in {brute_force_time:.2f} seconds.\")\n",
    "\n",
    "    return true_knn, brute_force_time\n",
    "\n",
    "# Ask the User if They Want to Use Existing Brute-Force Results\n",
    "if os.path.exists(\"true_knn.npy\") and os.path.exists(\"brute_time.npy\"):\n",
    "    user_input = input(\"Brute-Force K-NN results found. Do you want to use them? (yes/no): \").strip().lower()\n",
    "    \n",
    "    if user_input == \"yes\":\n",
    "        print(\"Loading precomputed Brute-Force results...\")\n",
    "        true_knn = np.load(\"true_knn.npy\", allow_pickle=True)\n",
    "        brute_force_time = np.load(\"brute_time.npy\")\n",
    "    else:\n",
    "        print(\"Recomputing Brute-Force K-NN...\")\n",
    "        true_knn, brute_force_time = brute_force_knn_optimized(train_vectors, test_vectors, k=7)\n",
    "        np.save(\"true_knn.npy\", true_knn)\n",
    "        np.save(\"brute_time.npy\", brute_force_time)\n",
    "        print(\"Brute-Force results saved for future use.\")\n",
    "else:\n",
    "    print(\"Brute-Force results not found. Running computation...\")\n",
    "    true_knn, brute_force_time = brute_force_knn_optimized(train_vectors, test_vectors, k=7)\n",
    "    np.save(\"true_knn.npy\", true_knn)\n",
    "    np.save(\"brute_time.npy\", brute_force_time)\n",
    "    print(\"Brute-Force results saved for future use.\")\n",
    "\n",
    "# Debug: Print first 10 rows of true KNN\n",
    "print(\"Sample of True K-NN Results (First 10 test samples):\")\n",
    "for i in range(10):\n",
    "    print(f\"Test sample {i} K-NN: {true_knn[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963bac8d-5712-43f0-96fb-c4749ee0ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorized data and Brute-Force K-NN results...\n",
      "All required data loaded.\n",
      "Evaluating LSH...\n",
      "Starting LSH with 16 permutations...\n",
      "Using threshold=0.85 for num_permutations=16\n",
      "LSH index built in 189.20 seconds.\n",
      "train_vectors shape: (111795, 5000)\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "LSH querying completed in 0.47 seconds.\n",
      "Computing fraction of true K-NN retrieved...\n",
      "Starting LSH with 32 permutations...\n",
      "Using threshold=0.9 for num_permutations=32\n",
      "LSH index built in 201.91 seconds.\n",
      "train_vectors shape: (111795, 5000)\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "LSH querying completed in 0.21 seconds.\n",
      "Computing fraction of true K-NN retrieved...\n",
      "Starting LSH with 64 permutations...\n",
      "Using threshold=0.9 for num_permutations=64\n",
      "LSH index built in 233.95 seconds.\n",
      "train_vectors shape: (111795, 5000)\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "Warning: Empty vector, inserting placeholder hash.\n",
      "LSH querying completed in 0.29 seconds.\n",
      "Computing fraction of true K-NN retrieved...\n",
      "\n",
      "Results Table:\n",
      "               Type  BuildTime          QueryTime          TotalTime Fraction              \n",
      "Parameters\n",
      "Brute-Force-Jaccard       0.00 133483.86831116676 133483.86831116676     100%                       -\n",
      "        LSH-Jaccard     189.20               0.47             189.67    2.81% Perm=16, Threshold=0.85\n",
      "        LSH-Jaccard     201.91               0.21             202.12    1.76%  Perm=32, Threshold=0.9\n",
      "        LSH-Jaccard     233.95               0.29             234.24    1.68%  Perm=64, Threshold=0.9\n",
      "Results saved to 'lsh_brute_force_comparison.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.sparse\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# Load Preprocessed Data\n",
    "print(\"Loading vectorized data and Brute-Force K-NN results...\")\n",
    "train_vectors = scipy.sparse.load_npz(\"train_vectors.npz\").toarray().astype(bool)\n",
    "test_vectors = scipy.sparse.load_npz(\"test_vectors.npz\").toarray().astype(bool)\n",
    "true_knn = np.load(\"true_knn.npy\", allow_pickle=True)\n",
    "brute_force_time = np.load(\"brute_time.npy\")\n",
    "print(\"All required data loaded.\")\n",
    "\n",
    "# Helper Functions\n",
    "def create_minhash(text_vector, num_permutations):\n",
    "    \"\"\"Creates a MinHash for a text vector, handling both 1D and 2D formats.\"\"\"\n",
    "    minhash = MinHash(num_perm=num_permutations)\n",
    "    \n",
    "    # Check correct handling of sparse row extraction\n",
    "    indices = text_vector.nonzero()  # Get nonzero features\n",
    "\n",
    "    if len(indices) == 1:  # Handle 1D case\n",
    "        indices = indices[0]\n",
    "    elif len(indices) > 1:  # Handle 2D case\n",
    "        indices = indices[1]\n",
    "    else:\n",
    "        indices = []\n",
    "\n",
    "    if len(indices) == 0:\n",
    "        print(\"Warning: Empty vector, inserting placeholder hash.\")\n",
    "        minhash.update(b'empty')  # Avoid zero hash issues\n",
    "    else:\n",
    "        for index in indices:\n",
    "            minhash.update(str(index).encode('utf8'))\n",
    "    \n",
    "    return minhash\n",
    "\n",
    "\n",
    "def compute_fraction_retrieved(true_knn, lsh_results):\n",
    "    \"\"\"Compute the fraction of true K-NN retrieved by LSH.\"\"\"\n",
    "    print(\"Computing fraction of true K-NN retrieved...\")\n",
    "    fractions = []\n",
    "    for true, retrieved in zip(true_knn, lsh_results):\n",
    "        if retrieved:\n",
    "            true_set = set(int(idx) for idx in np.ravel(true))\n",
    "            retrieved_set = set(map(int, retrieved))\n",
    "            overlap = len(true_set.intersection(retrieved_set))\n",
    "            fractions.append(overlap / len(true_set))\n",
    "        else:\n",
    "            fractions.append(0)\n",
    "    return np.mean(fractions)\n",
    "\n",
    "# LSH Implementation\n",
    "def lsh_knn_with_fraction(train_vectors, test_vectors, num_permutations, true_knn, k=7, threshold=0.2):\n",
    "    \"\"\"Run LSH, compute K-NN for the test set, and calculate fraction retrieved.\"\"\"\n",
    "    print(f\"Starting LSH with {num_permutations} permutations...\")\n",
    "    \n",
    "    print(f\"Using threshold={threshold} for num_permutations={num_permutations}\")\n",
    "\n",
    "    # Build LSH\n",
    "    lsh_start_time = time.time()\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_permutations)\n",
    "    train_minhashes = [create_minhash(vector, num_permutations) for vector in train_vectors]\n",
    "    for idx, minhash in enumerate(train_minhashes):\n",
    "        lsh.insert(str(idx), minhash)\n",
    "    build_time = time.time() - lsh_start_time\n",
    "    print(f\"LSH index built in {build_time:.2f} seconds.\")\n",
    "\n",
    "    print(f\"train_vectors shape: {train_vectors.shape}\")  # Should be (num_samples, num_features)\n",
    "    #print(f\"First row shape: {train_vectors[0].shape}\")  # Should be (num_features,)\n",
    "\n",
    "    # Precompute MinHashes for test vectors\n",
    "    test_minhashes = [create_minhash(vector, num_permutations) for vector in test_vectors]\n",
    "\n",
    "    # Query LSH\n",
    "    query_start_time = time.time()\n",
    "    lsh_results = [lsh.query(test_minhash) for test_minhash in test_minhashes]\n",
    "    #lsh_results = [sorted(lsh.query(test_minhash), key=int)[:100] for test_minhash in test_minhashes]\n",
    "    query_time = time.time() - query_start_time\n",
    "    print(f\"LSH querying completed in {query_time:.2f} seconds.\")\n",
    "\n",
    "    # Debug: Print how many neighbors are retrieved\n",
    "    # print(\"\\nChecking LSH Query Results (First 5 test samples)...\")\n",
    "    # for i in range(5):\n",
    "    #     print(f\"Test sample {i} retrieved {len(lsh_results[i])}\")\n",
    "\n",
    "    # Calculate the fraction of true K-NN retrieved\n",
    "    fraction_retrieved = compute_fraction_retrieved(true_knn, lsh_results)\n",
    "\n",
    "    return build_time, query_time, build_time + query_time, fraction_retrieved\n",
    "\n",
    "# Evaluate LSH\n",
    "print(\"Evaluating LSH...\")\n",
    "permutations_list = [16, 32, 64]\n",
    "results = []\n",
    "\n",
    "# Add Brute-Force results\n",
    "results.append({\n",
    "    \"Type\": \"Brute-Force-Jaccard\",\n",
    "    \"BuildTime\": 0,\n",
    "    \"QueryTime\": brute_force_time,\n",
    "    \"TotalTime\": brute_force_time,\n",
    "    \"Fraction\": \"100%\",\n",
    "    \"Parameters\": \"-\"\n",
    "})\n",
    "\n",
    "thresholds = {16: 0.85, 32: 0.9, 64: 0.9}\n",
    "\n",
    "for perm in permutations_list:\n",
    "    threshold = thresholds[perm]    \n",
    "    build_time, query_time, total_time, fraction = lsh_knn_with_fraction(\n",
    "        train_vectors, test_vectors, num_permutations=perm, true_knn=true_knn, k=7, threshold=threshold\n",
    "    )\n",
    "    results.append({\n",
    "        \"Type\": \"LSH-Jaccard\",\n",
    "        \"BuildTime\": round(build_time, 2),\n",
    "        \"QueryTime\": round(query_time, 2),\n",
    "        \"TotalTime\": round(total_time, 2),\n",
    "        \"Fraction\": f\"{round(fraction * 100, 2)}%\",\n",
    "        \"Parameters\": f\"Perm={perm}, Threshold={threshold}\"\n",
    "    })\n",
    "\n",
    "# Save Results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print table\n",
    "results_df = results_df[[\"Type\", \"BuildTime\", \"QueryTime\", \"TotalTime\", \"Fraction\", \"Parameters\"]]\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(results_df.to_string(index=False).replace('Parameters', '\\nParameters'))\n",
    "results_df.to_csv(\"lsh_brute_force_comparison.csv\", index=False)\n",
    "print(\"Results saved to 'lsh_brute_force_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621379e-c48d-4b9a-b2b6-ac608b458caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
